{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kairos1024/professional-projects/blob/main/Phi_3_Finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install huggingface-hub\n",
        "!pip install evaluate accelerate\n",
        "!pip install einops datasets bitsandbytes accelerate peft flash_attn\n",
        "!pip install tqdm\n"
      ],
      "metadata": {
        "id": "wqDFE7u5yPC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, BatchEncoding, BitsAndBytesConfig, get_linear_schedule_with_warmup\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import notebook_login, HfApi\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "import logging\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from accelerate import Accelerator\n"
      ],
      "metadata": {
        "id": "kLwsAztVM3-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Log in to Hugging Face\n",
        "!git config --global credential.helper store\n",
        "!huggingface-cli login\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "def print_runtime():\n",
        "    elapsed_time = time.time() - start_time\n",
        "    hours, rem = divmod(elapsed_time, 3600)\n",
        "    minutes, seconds = divmod(rem, 60)\n",
        "    print(f\"Session runtime: {int(hours):02}:{int(minutes):02}:{int(seconds):02}\")\n",
        "\n",
        "print(\"Starting the training script...\")\n",
        "print_runtime()\n",
        "\n",
        "# Configuration to load model in 4-bit quantized\n",
        "bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                                bnb_4bit_quant_type='nf4',\n",
        "                                bnb_4bit_compute_dtype='float16',\n",
        "                                bnb_4bit_use_double_quant=True)\n",
        "\n",
        "print(\"Loading the Phi-3 model and tokenizer...\")\n",
        "print_runtime()\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\", quantization_config=bnb_config, trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")\n",
        "\n",
        "# Enable gradient checkpointing to save memory\n",
        "model.gradient_checkpointing_enable()\n",
        "print(\"Enabled gradient checkpointing.\")\n",
        "print_runtime()\n",
        "\n",
        "# Freeze base model layers and cast layernorm in fp32\n",
        "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
        "print(\"Prepared the model for k-bit training.\")\n",
        "print_runtime()\n",
        "\n",
        "# Define the target modules for LORA\n",
        "target_modules = [f'model.layers.{i}.self_attn.o_proj' for i in range(32)] + \\\n",
        "                 [f'model.layers.{i}.self_attn.qkv_proj' for i in range(32)] + \\\n",
        "                 [f'model.layers.{i}.mlp.gate_up_proj' for i in range(32)] + \\\n",
        "                 [f'model.layers.{i}.mlp.down_proj' for i in range(32)]\n",
        "\n",
        "# Apply LORA\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=target_modules,\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "lora_model = get_peft_model(model, config)\n",
        "print(\"Applied LORA to the model.\")\n",
        "print_runtime()\n",
        "\n",
        "# Load the Schema-Guided Dialogue dataset\n",
        "print(\"Loading the Schema-Guided Dialogue dataset...\")\n",
        "print_runtime()\n",
        "dataset = load_dataset(\"schema_guided_dstc8\", \"dialogues\", split='train')\n",
        "\n",
        "# Define a function to preprocess the examples\n",
        "def preprocess(example, max_turns=10):\n",
        "    dialogue = []\n",
        "    speakers = example['turns']['speaker'][:max_turns]\n",
        "    utterances = example['turns']['utterance'][:max_turns]\n",
        "    frames = example['turns']['frames'][:max_turns]\n",
        "\n",
        "    for i in range(len(speakers)):\n",
        "        speaker = \"User\" if speakers[i] == 0 else \"System\"\n",
        "        utterance = utterances[i]\n",
        "        frame = frames[i]\n",
        "\n",
        "        state_info = \"\"\n",
        "        if 'state' in frame and len(frame['state']) > 0:\n",
        "            state = frame['state'][0]\n",
        "            active_intent = state.get('active_intent', 'None')\n",
        "            slots = state.get('slot_values', {})\n",
        "            state_info = f\" [intent: {active_intent}] [slots: {slots}]\"\n",
        "\n",
        "        action_info = \"\"\n",
        "        if 'actions' in frame and len(frame['actions']) > 0:\n",
        "            actions = frame['actions'][0]\n",
        "            act = actions.get('act', [])\n",
        "            slot = actions.get('slot', [])\n",
        "            values = actions.get('values', [])\n",
        "            action_info = f\" [actions: {act}] [slots: {slot}] [values: {values}]\"\n",
        "\n",
        "        dialogue.append(f\"{speaker}: {utterance}{state_info}{action_info}\")\n",
        "\n",
        "    tokenized_dialogue = tokenizer(\n",
        "        ' '.join(dialogue),\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=256  # Adjust sequence length if necessary\n",
        "    )\n",
        "\n",
        "    # Labels are the same as input_ids for causal language modeling\n",
        "    tokenized_dialogue[\"labels\"] = tokenized_dialogue[\"input_ids\"].copy()\n",
        "\n",
        "    return {key: torch.tensor(val) for key, val in tokenized_dialogue.items()}\n",
        "\n",
        "# Preprocess the whole dataset\n",
        "print(\"Preprocessing the dataset...\")\n",
        "preprocessed_dataset = [preprocess(example) for example in tqdm(dataset)]\n",
        "print(f\"Dataset size after preprocessing: {len(preprocessed_dataset)}\")\n",
        "print_runtime()\n",
        "\n",
        "# Convert the preprocessed data into a Dataset object\n",
        "class PreprocessedDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "tokenized_dataset = PreprocessedDataset(preprocessed_dataset)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_size = int(0.8 * len(tokenized_dataset))\n",
        "val_size = len(tokenized_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(tokenized_dataset, [train_size, val_size])\n",
        "print(f\"Training set size: {train_size}\")\n",
        "print(f\"Validation set size: {val_size}\")\n",
        "print_runtime()\n",
        "\n",
        "# Define a function to compute metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = (predictions == labels).mean()\n",
        "    return {\"accuracy\": accuracy}\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=2,  # Reduce batch size\n",
        "    per_device_eval_batch_size=2,   # Match evaluation batch size\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    gradient_accumulation_steps=8,  # Increase gradient accumulation steps\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    load_best_model_at_end=True,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        ")\n",
        "\n",
        "# Initialize Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "# Prepare the model, optimizer, and dataloaders with Accelerator\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=training_args.per_device_train_batch_size, shuffle=True, num_workers=4)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=training_args.per_device_eval_batch_size, shuffle=False, num_workers=4)\n",
        "optimizer = torch.optim.AdamW(lora_model.parameters(), lr=training_args.learning_rate)\n",
        "\n",
        "lora_model, optimizer, train_dataloader, val_dataloader = accelerator.prepare(\n",
        "    lora_model, optimizer, train_dataloader, val_dataloader\n",
        ")\n",
        "\n",
        "# Training loop with Accelerator\n",
        "for epoch in range(training_args.num_train_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{training_args.num_train_epochs}\")\n",
        "    lora_model.train()\n",
        "    progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=\"Training\", leave=True)\n",
        "    for step, batch in progress_bar:\n",
        "        batch = {k: v.to(accelerator.device) for k, v in batch.items()}\n",
        "        outputs = lora_model(**batch)\n",
        "        loss = outputs.loss\n",
        "        accelerator.backward(loss)\n",
        "\n",
        "        if step % training_args.gradient_accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item())})\n",
        "\n",
        "    # Validation loop\n",
        "    lora_model.eval()\n",
        "    val_progress_bar = tqdm(val_dataloader, desc=\"Validation\", leave=True)\n",
        "    for batch in val_progress_bar:\n",
        "        batch = {k: v.to(accelerator.device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = lora_model(**batch)\n",
        "\n",
        "print(\"Training completed.\")\n",
        "print_runtime()\n",
        "\n",
        "# Upload the final model to Hugging Face\n",
        "repo_name = \"your-repo-name\"\n",
        "api = HfApi()\n",
        "api.create_repo(repo_name, exist_ok=True)\n",
        "\n",
        "# Push final model and tokenizer files to Hugging Face Model Hub\n",
        "print(f\"Pushing the final model and tokenizer to the Hugging Face Hub under the repository {repo_name}...\")\n",
        "!git clone https://huggingface.co/username/model-name\n",
        "\n",
        "!git config --global user.email \"your-email\"\n",
        "!git config --global user.name \"your-username\"\n",
        "\n",
        "# Save the model and tokenizer files\n",
        "lora_model.save_pretrained(\"model-name\")\n",
        "tokenizer.save_pretrained(\".model-name\")\n",
        "\n",
        "# Change directory to the cloned repository, add, commit, and push in a single command\n",
        "!cd \"model-name\" && git add . && git commit -m \"Add model and tokenizer files\" && git push\n",
        "\n",
        "print(f\"Model and tokenizer saved to {repo_name}\")\n",
        "\n",
        "# Print GPU usage\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory}\")\n",
        "    print(f\"Current GPU memory allocated: {torch.cuda.memory_allocated()}\")\n",
        "    print(f\"Current GPU memory cached: {torch.cuda.memory_reserved()}\")"
      ],
      "metadata": {
        "id": "g03DYrRpzQV2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1JTX6fOloL3TTGXXPbH04elAv0r9ZJ_sd",
      "authorship_tag": "ABX9TyPx+V7JI93h0jq2yzbelYSP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
